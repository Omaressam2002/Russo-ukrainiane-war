{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8419277,"sourceType":"datasetVersion","datasetId":5012023},{"sourceId":8435945,"sourceType":"datasetVersion","datasetId":5008857},{"sourceId":8507658,"sourceType":"datasetVersion","datasetId":5078291},{"sourceId":48928,"sourceType":"modelInstanceVersion","modelInstanceId":40904},{"sourceId":48933,"sourceType":"modelInstanceVersion","modelInstanceId":40909},{"sourceId":55782,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":40902}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":57.981547,"end_time":"2024-05-16T20:04:37.108811","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-16T20:03:39.127264","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.750054,"end_time":"2024-05-16T20:03:42.629026","exception":false,"start_time":"2024-05-16T20:03:41.878972","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T10:12:49.202534Z","iopub.execute_input":"2024-05-25T10:12:49.203416Z","iopub.status.idle":"2024-05-25T10:12:50.085918Z","shell.execute_reply.started":"2024-05-25T10:12:49.203374Z","shell.execute_reply":"2024-05-25T10:12:50.084672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gensim==3.4.0\n!pip install smart_open==1.9.0","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-05-16T20:03:42.654177Z","iopub.status.busy":"2024-05-16T20:03:42.653697Z","iopub.status.idle":"2024-05-16T20:04:33.943478Z","shell.execute_reply":"2024-05-16T20:04:33.942481Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"papermill":{"duration":51.305029,"end_time":"2024-05-16T20:04:33.945880","exception":false,"start_time":"2024-05-16T20:03:42.640851","status":"completed"},"tags":[],"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-05-16T07:46:53.544441Z","iopub.status.busy":"2024-05-16T07:46:53.543742Z","iopub.status.idle":"2024-05-16T07:46:56.442814Z","shell.execute_reply":"2024-05-16T07:46:56.441773Z","shell.execute_reply.started":"2024-05-16T07:46:53.544407Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"papermill":{"duration":0.015053,"end_time":"2024-05-16T20:04:33.976518","exception":false,"start_time":"2024-05-16T20:04:33.961465","status":"completed"},"tags":[],"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pretraining**","metadata":{"papermill":{"duration":0.014927,"end_time":"2024-05-16T20:04:34.007662","exception":false,"start_time":"2024-05-16T20:04:33.992735","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport re\n\ntweets_sg_100 = '/kaggle/input/tweets-sg-100/tweets_sg_100'\nsg_model = gensim.models.Word2Vec.load(tweets_sg_100)\ndf = pd.read_csv(\"/kaggle/input/tweets-pipeline/tweets_0.csv\")\ndf.head()","metadata":{"papermill":{"duration":2.547892,"end_time":"2024-05-16T20:04:36.571119","exception":true,"start_time":"2024-05-16T20:04:34.023227","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T10:12:50.087864Z","iopub.execute_input":"2024-05-25T10:12:50.088903Z","iopub.status.idle":"2024-05-25T10:13:14.191921Z","shell.execute_reply.started":"2024-05-25T10:12:50.088863Z","shell.execute_reply":"2024-05-25T10:13:14.190722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"russia = ['روسي','روس','روسيه','لروسي','والروس','ياروسي','روسی','موسك'] # 'روسيا'\nbelarus = ['يلاروسي','يلاروس','بيلاروس','بيلاروسيه','لبيلاروسي','والبيلاروس'] # 'بيلاروسيا'\nukr = ['اوكران','واوكراني','اوكر','لاوكراني','ناتو'] # 'اوكرانيا'\nrussian_hashtags = ['جيشالروس','قواتالروسيه','روسياليوم','روسياتنتصر','خارجيهالروسيه','عمليهالعسكريهالروسيه'] # روسيا\nukr_russian_hashtags = ['حربالروسيهالاوكرانيه','روسياوكراني','اوكرانياروسي','غزوالروسيلاوكرانيا','روسياواوكراني'] # 'حرب'\nukr_hashtags = ['روسياتغزواوكراني','روسياتكذب','روسيالص','روسياوالص','روسيامجرم','مجدلاوكرانيا','مخابراتالاوكرانيه'] # 'اوكرانيا'\nzelensky = ['زيلينسك','زلينسك'] # 'اوباما'\nputin = ['بوتن','وتن','پوتن','وتين','بوتين','پوتين'] #'بوتين'\nwar = ['حربالعالميه', 'حربالعالميهالثالثه','عسكريه'] # 'حرب'\negypt = ['مصر','مصريه','لمصر'] # 'عربي'\nus = ['وامريك','الولاياتالمتحده','الولاياتالمتحده','الولاياتالمتحدهالامريكيه','البريطانيا','والولايا'] # 'امريكا'\ngb = ['ريطاني'] # \"بريطانيا\"\n\nrussia.extend(russian_hashtags)\nukr.extend(ukr_hashtags)\nwar.extend(ukr_russian_hashtags)\n\nadded_words = []\nadded_words.extend(russia)\nadded_words.extend(ukr)\nadded_words.extend(war)\nadded_words.extend(putin)\nadded_words.extend(zelensky)\nadded_words.extend(egypt)\nadded_words.extend(us)\nadded_words.extend(gb)\nadded_words = set(added_words)\nrussia = set(russia)\nukr = set(ukr)\nwar = set(war)\nputin = set(putin)\nzelensky = set(zelensky)\negypt = set(egypt)\nus = set(us)\ngb = set(gb)\n\n# add words that donot exist to wv\n# russia 'روسيا', ukr 'اوكرانيا', war 'حرب' , putin 'بوتين', zelensky 'اوباما', belarus 'بيلاروسيا' , egypt 'قطر', us 'امريكا', gb \"بريطانيا\"\nfor word in russia:\n    sg_model.wv.add_vector(word,sg_model.wv['روسيا'])\nfor word in ukr:\n    sg_model.wv.add_vector(word,sg_model.wv['اوكرانيا'])\nfor word in war:\n    sg_model.wv.add_vector(word,sg_model.wv['حرب'])\nfor word in putin:\n    sg_model.wv.add_vector(word,sg_model.wv['بوتين'])\nfor word in zelensky:\n    sg_model.wv.add_vector(word,sg_model.wv['اوباما'])\nfor word in belarus:\n    sg_model.wv.add_vector(word,sg_model.wv['بيلاروسيا'])\nfor word in egypt:\n    sg_model.wv.add_vector(word,sg_model.wv['قطر'])   \nfor word in us:\n    sg_model.wv.add_vector(word,sg_model.wv['امريكا']) \nfor word in gb:\n    sg_model.wv.add_vector(word,sg_model.wv[\"بريطانيا\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-25T10:13:24.130607Z","iopub.execute_input":"2024-05-25T10:13:24.131455Z","iopub.status.idle":"2024-05-25T10:13:27.213607Z","shell.execute_reply.started":"2024-05-25T10:13:24.131421Z","shell.execute_reply":"2024-05-25T10:13:27.212426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ndef load_pretraining(file_path):\n    df = pd.read_csv(file_path)\n    sentences = df[\"tweet_text\"].astype(str)\n    ex_length = len(df)\n    X1 = tf.zeros([ex_length,64,100],dtype=tf.float32)\n    X2 = tf.zeros([ex_length,64,100],dtype=tf.float32)\n    for ex in range(ex_length):\n        words = sentences[ex].split()\n        position = 0\n        for word in words:\n            if word in sg_model.wv:\n                X1[ex,position,:] = sg_model.wv[word]\n                X2[ex,position+1,:].assign(sg_model.wv[word])\n    print(X1.shape,X2.shape)     \n    return (X1,X2),X1","metadata":{"papermill":{"duration":2.547892,"end_time":"2024-05-16T20:04:36.571119","exception":true,"start_time":"2024-05-16T20:04:34.023227","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T11:50:38.226898Z","iopub.execute_input":"2024-05-25T11:50:38.227297Z","iopub.status.idle":"2024-05-25T11:50:38.235976Z","shell.execute_reply.started":"2024-05-25T11:50:38.227266Z","shell.execute_reply":"2024-05-25T11:50:38.234578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_paths = []\nfor i in range(14):\n    file_paths.append(\"/kaggle/input/tweets-pipeline/tweets_\"+str(i)+\".csv\")","metadata":{"papermill":{"duration":2.547892,"end_time":"2024-05-16T20:04:36.571119","exception":true,"start_time":"2024-05-16T20:04:34.023227","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T11:12:38.711372Z","iopub.execute_input":"2024-05-25T11:12:38.712200Z","iopub.status.idle":"2024-05-25T11:12:38.716650Z","shell.execute_reply.started":"2024-05-25T11:12:38.712161Z","shell.execute_reply":"2024-05-25T11:12:38.715715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tweet_generator_pretraining():\n    for file_path in file_paths:\n        X,y = load_pretraining(file_path)\n        for tweet,label in zip(X,y):\n            yield tweet,label","metadata":{"papermill":{"duration":2.547892,"end_time":"2024-05-16T20:04:36.571119","exception":true,"start_time":"2024-05-16T20:04:34.023227","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T11:50:40.150504Z","iopub.execute_input":"2024-05-25T11:50:40.150890Z","iopub.status.idle":"2024-05-25T11:50:40.155827Z","shell.execute_reply.started":"2024-05-25T11:50:40.150863Z","shell.execute_reply":"2024-05-25T11:50:40.154929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = load_pretraining(\"/kaggle/input/tweets-pipeline/tweets_9.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-25T11:54:30.686363Z","iopub.execute_input":"2024-05-25T11:54:30.687132Z","iopub.status.idle":"2024-05-25T11:54:41.267250Z","shell.execute_reply.started":"2024-05-25T11:54:30.687101Z","shell.execute_reply":"2024-05-25T11:54:41.265878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a TensorFlow Dataset\nimport tensorflow as tf\npretraining_dataset = tf.data.Dataset.from_generator(tweet_generator_pretraining,output_signature=\n                                                     ((tf.TensorSpec(shape=(64, 100), dtype=tf.float32),\n                                                     tf.TensorSpec(shape=(64, 100), dtype=tf.float32)), \n                                                      tf.TensorSpec(shape=(64,100), dtype=tf.float32)))                                                                           \nbatch_size = 8\ndataset = pretraining_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)","metadata":{"papermill":{"duration":2.547892,"end_time":"2024-05-16T20:04:36.571119","exception":true,"start_time":"2024-05-16T20:04:34.023227","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T11:35:32.453690Z","iopub.execute_input":"2024-05-25T11:35:32.454070Z","iopub.status.idle":"2024-05-25T11:35:32.498935Z","shell.execute_reply.started":"2024-05-25T11:35:32.454041Z","shell.execute_reply":"2024-05-25T11:35:32.498184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (encoder_input, decoder_input), target_output in pretraining_dataset.take(1):\n    print('Encoder input shape:', encoder_input.shape)\n    print('Decoder input shape:', decoder_input.shape)\n    print('Target output shape:', target_output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T11:35:35.312206Z","iopub.execute_input":"2024-05-25T11:35:35.312981Z","iopub.status.idle":"2024-05-25T11:36:09.384432Z","shell.execute_reply.started":"2024-05-25T11:35:35.312950Z","shell.execute_reply":"2024-05-25T11:36:09.382303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Bidirectional, LSTM, Dense, Flatten\n\n# Define an input sequence and process it.\nencoder_inputs = Input(shape=(None, 100),name='e_inp') \ndecoder_inputs = Input(shape=(None, 100),name='d_inp')\nencoder = Bidirectional(LSTM(64, return_state=True),name='enc')\nencoder_outputs, hf, hb, cf, cb = encoder(encoder_inputs)\nencoder_states = [hf, hb, cf, cb]\ndecoder_lstm = Bidirectional(LSTM(64, return_sequences=True, return_state=True),name='dec')\ndecoder_outputs, dhf, dhb, dcf, dcb = decoder_lstm(decoder_inputs,initial_state=encoder_states)\ndecoder_dense = Dense(100,name=\"dense_1\")\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.summary()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T11:01:11.689669Z","iopub.execute_input":"2024-05-25T11:01:11.690570Z","iopub.status.idle":"2024-05-25T11:01:11.802277Z","shell.execute_reply.started":"2024-05-25T11:01:11.690532Z","shell.execute_reply":"2024-05-25T11:01:11.801348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"/kaggle/input/best_weights/keras/best_best/2/pretrained_weights.weights.h5\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T23:17:22.282330Z","iopub.execute_input":"2024-05-24T23:17:22.283165Z","iopub.status.idle":"2024-05-24T23:17:22.356605Z","shell.execute_reply.started":"2024-05-24T23:17:22.283135Z","shell.execute_reply":"2024-05-24T23:17:22.355826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.losses import CosineSimilarity, MeanSquaredError\ncs_loss = CosineSimilarity(axis=-1, reduction=\"sum_over_batch_size\", name=\"cosine_similarity\")\nmodel.compile(optimizer='adam',loss=cs_loss ,metrics=['acc'])\nmodel.fit(pretrainig_dataset,\n          epochs=10)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T11:02:35.708381Z","iopub.execute_input":"2024-05-25T11:02:35.709024Z","iopub.status.idle":"2024-05-25T11:02:35.794313Z","shell.execute_reply.started":"2024-05-25T11:02:35.708989Z","shell.execute_reply":"2024-05-25T11:02:35.793175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"pre_model_bilstm.weights.h5\")","metadata":{"execution":{"iopub.execute_input":"2024-05-16T16:30:12.379253Z","iopub.status.busy":"2024-05-16T16:30:12.378903Z","iopub.status.idle":"2024-05-16T16:30:12.454017Z","shell.execute_reply":"2024-05-16T16:30:12.453200Z","shell.execute_reply.started":"2024-05-16T16:30:12.379224Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Partiality** ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"encoder = model.get_layer('enc')\nconfig = encoder.get_config()\nconfig['layer']['config']['return_state'] = False\nconfig['layer']['config']['return_sequences'] = True\n\nencoder_layer_new = Bidirectional(LSTM.from_config(config['layer']['config']), name='enc')\n\n# Get the weights of the original encoder layer and set them to the new layer\nencoder_layer_new.build((None,None,100))\nencoder_layer_new.set_weights(encoder.get_weights())       ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T23:17:39.818128Z","iopub.execute_input":"2024-05-24T23:17:39.818561Z","iopub.status.idle":"2024-05-24T23:17:39.872481Z","shell.execute_reply.started":"2024-05-24T23:17:39.818530Z","shell.execute_reply":"2024-05-24T23:17:39.871621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Bidirectional, LSTM, Dense\nfrom tensorflow.keras.losses import CategoricalCrossentropy\n\n# Define the model\nmodel = Sequential()\n# Add Bidirectional LSTM layers\nmodel.add(encoder_layer_new)\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n\n# Add Dense layer\nmodel.add(Dense(units=20, activation='relu'))\n\n# Add Softmax layer\nmodel.add(Dense(units=3, activation='softmax'))\n\nloss= CategoricalCrossentropy(\n    from_logits=False,\n    label_smoothing=0.0,\n    axis=-1,\n    reduction=\"sum_over_batch_size\",\n    name=\"categorical_crossentropy\",\n)\n# Compile the model\nmodel.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-24T23:17:41.196470Z","iopub.execute_input":"2024-05-24T23:17:41.196824Z","iopub.status.idle":"2024-05-24T23:17:41.243981Z","shell.execute_reply.started":"2024-05-24T23:17:41.196795Z","shell.execute_reply":"2024-05-24T23:17:41.243177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_preprocess_tweets(file_path):\n    df = pd.read_csv(file_path)\n    sentences = df[\"tweet_text\"].astype(str)\n    y= to_categorical(df['label'])\n    ex_length = len(df)\n    X = np.zeros([ex_length,64,100])\n    for ex in range(ex_length):\n        words = sentences[ex].split()\n        position = 0\n        for word in words:\n            if word in sg_model.wv:\n                X[ex,position,:] = sg_model.wv[word]\n            position += 1\n       \n    return X,y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tweet_generator():\n    for file_path in file_paths:\n        X, y = load_and_preprocess_tweets(file_path)\n        for tweet, label in zip(X, y):\n            yield tweet, label","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a TensorFlow Dataset\nimport tensorflow as tf\ndataset = tf.data.Dataset.from_generator(tweet_generator,output_signature=(tf.TensorSpec(shape=(None,100), dtype=tf.float32), tf.TensorSpec(shape=(3,), dtype=tf.float32)))                                                                           \nbatch_size = 32\ndataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val, y_val = load_and_preprocess_tweets(\"/kaggle/input/tweets-pipeline/tweets_12.csv\")\nX_test, y_test = load_and_preprocess_tweets(\"/kaggle/input/tweets-pipeline/tweets_13.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\n# Define the checkpoint filepath with '.keras' extension\ncheckpoint_filepath = 'best_weights.keras'\n\n# Define the ModelCheckpoint callback\ncheckpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n                                      save_best_only=True,\n                                      monitor='val_accuracy',\n                                      mode='max',  # 'max' for accuracy\n                                      verbose=1)\n\n# Train the model with ModelCheckpoint callback\nhistory = model.fit(dataset,\n                    epochs=1,\n                    callbacks=[checkpoint_callback],shuffle = True,validation_data=(X_val,y_val))\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T01:04:08.135682Z","iopub.execute_input":"2024-05-25T01:04:08.136052Z","iopub.status.idle":"2024-05-25T01:17:35.482805Z","shell.execute_reply.started":"2024-05-25T01:04:08.136023Z","shell.execute_reply":"2024-05-25T01:17:35.481853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"/kaggle/input/best_weights/keras/best_best/1/best_weights-9.keras\")","metadata":{"execution":{"iopub.execute_input":"2024-05-16T19:38:55.793276Z","iopub.status.busy":"2024-05-16T19:38:55.792571Z","iopub.status.idle":"2024-05-16T19:38:55.956331Z","shell.execute_reply":"2024-05-16T19:38:55.955254Z","shell.execute_reply.started":"2024-05-16T19:38:55.793242Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss,accuracy = model.evaluate(X_test,y_test)\n\nprint(f\"Loss :{loss:.4f}\")\nprint(f\"Accuracy :{accuracy:.4f}\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-05-25T01:23:35.045429Z","iopub.execute_input":"2024-05-25T01:23:35.046087Z","iopub.status.idle":"2024-05-25T01:23:39.042543Z","shell.execute_reply.started":"2024-05-25T01:23:35.046048Z","shell.execute_reply":"2024-05-25T01:23:39.041591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"/kaggle/input/normal22/keras/v1/1/model.weights.h5\")","metadata":{"execution":{"iopub.execute_input":"2024-05-16T19:59:00.107306Z","iopub.status.busy":"2024-05-16T19:59:00.106548Z","iopub.status.idle":"2024-05-16T19:59:00.250987Z","shell.execute_reply":"2024-05-16T19:59:00.249932Z","shell.execute_reply.started":"2024-05-16T19:59:00.107273Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model 4 is the best model\n# model.weights.h5","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# self training\n# pick tweet 0\n# and run prediction on it \n# and see the least confident predictions \n# get their indices and from indices get their \n# label them manually\n# same for x_test\n# ","metadata":{},"execution_count":null,"outputs":[]}]}