{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d738d419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T11:37:09.089971Z",
     "iopub.status.busy": "2024-05-16T11:37:09.089130Z",
     "iopub.status.idle": "2024-05-16T11:37:10.021711Z",
     "shell.execute_reply": "2024-05-16T11:37:10.020731Z",
     "shell.execute_reply.started": "2024-05-16T11:37:09.089938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tweets-sg-100/tweets_sg_100.trainables.syn1neg.npy\n",
      "/kaggle/input/tweets-sg-100/tweets_sg_100\n",
      "/kaggle/input/tweets-sg-100/tweets_sg_100.wv.vectors.npy\n",
      "/kaggle/input/partiality-scores/labeled.csv\n",
      "/kaggle/input/partiality-scores/label.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e54e40c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T11:37:10.024340Z",
     "iopub.status.busy": "2024-05-16T11:37:10.023772Z",
     "iopub.status.idle": "2024-05-16T11:37:39.769783Z",
     "shell.execute_reply": "2024-05-16T11:37:39.768905Z",
     "shell.execute_reply.started": "2024-05-16T11:37:10.024287Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/partiality-scores/labeled.csv\")\n",
    "sentences = df[\"preprocessed tweets\"].astype(str)\n",
    "\n",
    "tweets_sg_100 = '/kaggle/input/tweets-sg-100/tweets_sg_100'\n",
    "sg_model = gensim.models.Word2Vec.load(tweets_sg_100)\n",
    "# ANALYZE HASHTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23f1ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T11:37:39.793144Z",
     "iopub.status.busy": "2024-05-16T11:37:39.792766Z",
     "iopub.status.idle": "2024-05-16T11:37:39.831023Z",
     "shell.execute_reply": "2024-05-16T11:37:39.829988Z",
     "shell.execute_reply.started": "2024-05-16T11:37:39.793110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>partiality_score</th>\n",
       "      <th>preprocessed tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1641953510903234562</td>\n",
       "      <td>روس تحتج علي تصرف استفزازيه قوات امريكيه في سور</td>\n",
       "      <td>4.50</td>\n",
       "      <td>روس تحتج عل تصرف استفزاز قوا امريك في سور</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1641953254094381057</td>\n",
       "      <td>روس تحتج علي استفزاز قوات اميركيه في سور</td>\n",
       "      <td>4.50</td>\n",
       "      <td>روس تحتج عل استفزاز قوا اميرك في سور</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1641953209810862080</td>\n",
       "      <td>توضيح لن يحدث شي مهم هذا اسبوع ومن متوقع تبدا ...</td>\n",
       "      <td>-5.25</td>\n",
       "      <td>توضيح لن يحدث شي مهم هذا اسبوع ومن متوقع تبد ا...</td>\n",
       "      <td>pro russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1641953161295347716</td>\n",
       "      <td>فرنسا بتشوف ايام شرق اوسطيه افريقيه زي الفل يع...</td>\n",
       "      <td>-5.25</td>\n",
       "      <td>رنس تشوف ايام شرق اوسط افريق زي الفل يعن رييس ...</td>\n",
       "      <td>pro russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1641952955136974848</td>\n",
       "      <td>روس تحتج علي استفزاز قوات اميركيه في سور سكاي ...</td>\n",
       "      <td>4.50</td>\n",
       "      <td>روس تحتج عل استفزاز قوا اميرك في سور سكا نيوز عرب</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                         tweet_text  \\\n",
       "0  1641953510903234562    روس تحتج علي تصرف استفزازيه قوات امريكيه في سور   \n",
       "1  1641953254094381057           روس تحتج علي استفزاز قوات اميركيه في سور   \n",
       "2  1641953209810862080  توضيح لن يحدث شي مهم هذا اسبوع ومن متوقع تبدا ...   \n",
       "3  1641953161295347716  فرنسا بتشوف ايام شرق اوسطيه افريقيه زي الفل يع...   \n",
       "4  1641952955136974848  روس تحتج علي استفزاز قوات اميركيه في سور سكاي ...   \n",
       "\n",
       "   partiality_score                                preprocessed tweets  \\\n",
       "0              4.50          روس تحتج عل تصرف استفزاز قوا امريك في سور   \n",
       "1              4.50               روس تحتج عل استفزاز قوا اميرك في سور   \n",
       "2             -5.25  توضيح لن يحدث شي مهم هذا اسبوع ومن متوقع تبد ا...   \n",
       "3             -5.25  رنس تشوف ايام شرق اوسط افريق زي الفل يعن رييس ...   \n",
       "4              4.50  روس تحتج عل استفزاز قوا اميرك في سور سكا نيوز عرب   \n",
       "\n",
       "        label  \n",
       "0     neutral  \n",
       "1     neutral  \n",
       "2  pro russia  \n",
       "3  pro russia  \n",
       "4     neutral  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610531ec-d3ba-4c54-bf7b-eda224677a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T11:37:39.832572Z",
     "iopub.status.busy": "2024-05-16T11:37:39.832242Z",
     "iopub.status.idle": "2024-05-16T11:37:39.841440Z",
     "shell.execute_reply": "2024-05-16T11:37:39.840339Z",
     "shell.execute_reply.started": "2024-05-16T11:37:39.832547Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding important words to the embedder, words he didnt know about\n",
    "\n",
    "# important words not recognized by the embedder \n",
    "russia = ['روسي','روس','روسيه','لروسي','والروس','ياروسي','روسی','موسك'] # 'روسيا'\n",
    "belarus = ['يلاروسي','يلاروس','بيلاروس','بيلاروسيه','لبيلاروسي','والبيلاروس'] # 'بيلاروسيا'\n",
    "ukr = ['اوكران','واوكراني','اوكر','لاوكراني','ناتو'] # 'اوكرانيا'\n",
    "russian_hashtags = ['جيشالروس','قواتالروسيه','روسياليوم','روسياتنتصر','خارجيهالروسيه','عمليهالعسكريهالروسيه'] # روسيا\n",
    "ukr_russian_hashtags = ['حربالروسيهالاوكرانيه','روسياوكراني','اوكرانياروسي','غزوالروسيلاوكرانيا','روسياواوكراني'] # 'حرب'\n",
    "ukr_hashtags = ['روسياتغزواوكراني','روسياتكذب','روسيالص','روسياوالص','روسيامجرم','مجدلاوكرانيا','مخابراتالاوكرانيه'] # 'اوكرانيا'\n",
    "zelensky = ['زيلينسك','زلينسك'] # 'اوباما'\n",
    "putin = ['بوتن','وتن','پوتن','وتين','بوتين','پوتين'] #'بوتين'\n",
    "war = ['حربالعالميه', 'حربالعالميهالثالثه','عسكريه'] # 'حرب'\n",
    "egypt = ['مصر','مصريه','لمصر'] # 'عربي'\n",
    "us = ['وامريك','الولاياتالمتحده','الولاياتالمتحده','الولاياتالمتحدهالامريكيه','البريطانيا','والولايا','امريك'] # 'امريكا'\n",
    "gb = ['ريطاني'] # \"بريطانيا\"\n",
    "\n",
    "russia.extend(russian_hashtags)\n",
    "ukr.extend(ukr_hashtags)\n",
    "war.extend(ukr_russian_hashtags)\n",
    "\n",
    "added_words = []\n",
    "added_words.extend(russia)\n",
    "added_words.extend(ukr)\n",
    "added_words.extend(war)\n",
    "added_words.extend(putin)\n",
    "added_words.extend(zelensky)\n",
    "added_words.extend(egypt)\n",
    "added_words.extend(us)\n",
    "added_words.extend(gb)\n",
    "added_words = set(added_words)\n",
    "russia = set(russia)\n",
    "ukr = set(ukr)\n",
    "war = set(war)\n",
    "putin = set(putin)\n",
    "zelensky = set(zelensky)\n",
    "egypt = set(egypt)\n",
    "us = set(us)\n",
    "gb = set(gb)\n",
    "\n",
    "# add words that donot exist to wv\n",
    "# russia 'روسيا', ukr 'اوكرانيا', war 'حرب' , putin 'بوتين', zelensky 'اوباما', belarus 'بيلاروسيا' , egypt 'قطر', us 'امريكا', gb \"بريطانيا\"\n",
    "for word in russia:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['روسيا'])\n",
    "for word in ukr:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['اوكرانيا'])\n",
    "for word in war:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['حرب'])\n",
    "for word in putin:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['بوتين'])\n",
    "for word in zelensky:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['اوباما'])\n",
    "for word in belarus:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['بيلاروسيا'])\n",
    "for word in egypt:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['قطر'])   \n",
    "for word in us:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv['امريكا']) \n",
    "for word in gb:\n",
    "    sg_model.wv.add_vector(word,sg_model.wv[\"بريطانيا\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f4e0092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:55:41.275805Z",
     "iopub.status.busy": "2024-05-16T12:55:41.275394Z",
     "iopub.status.idle": "2024-05-16T12:55:41.781077Z",
     "shell.execute_reply": "2024-05-16T12:55:41.779849Z",
     "shell.execute_reply.started": "2024-05-16T12:55:41.275775Z"
    }
   },
   "outputs": [],
   "source": [
    "ex_length = 100000\n",
    "max_len = max([ len(sentence.split()) for sentence in sentences ])\n",
    "encoder_input_data = np.zeros([ex_length,max_len+5,100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c9a5036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:24:18.011493Z",
     "iopub.status.busy": "2024-05-16T12:24:18.011034Z",
     "iopub.status.idle": "2024-05-16T12:24:31.594200Z",
     "shell.execute_reply": "2024-05-16T12:24:31.593061Z",
     "shell.execute_reply.started": "2024-05-16T12:24:18.011460Z"
    }
   },
   "outputs": [],
   "source": [
    "# russia 'روسيا', ukr 'اوكرانيا', war 'حرب' , putin 'بوتين', zelensky 'اوباما', belarus 'بيلاروسيا' , egypt 'قطر', us 'امريكا', gb \"بريطانيا\"\n",
    "for ex in range(100000,100000+ex_length):\n",
    "    words = sentences[ex].split()\n",
    "    position = 0\n",
    "    for word in words:\n",
    "        if word in sg_model.wv:\n",
    "            encoder_input_data[ex-100000,position,:] = sg_model.wv[word]\n",
    "        position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5f82d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:23:30.380004Z",
     "iopub.status.busy": "2024-05-16T12:23:30.379224Z",
     "iopub.status.idle": "2024-05-16T12:23:30.385528Z",
     "shell.execute_reply": "2024-05-16T12:23:30.384664Z",
     "shell.execute_reply.started": "2024-05-16T12:23:30.379974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219406,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9e17da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:24:46.272437Z",
     "iopub.status.busy": "2024-05-16T12:24:46.271769Z",
     "iopub.status.idle": "2024-05-16T12:24:46.277333Z",
     "shell.execute_reply": "2024-05-16T12:24:46.276436Z",
     "shell.execute_reply.started": "2024-05-16T12:24:46.272400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# labels are in the third column\n",
    "y = df['label'][100000:200000]\n",
    "\n",
    "x = encoder_input_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5856c400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:24:52.927301Z",
     "iopub.status.busy": "2024-05-16T12:24:52.926366Z",
     "iopub.status.idle": "2024-05-16T12:24:52.959500Z",
     "shell.execute_reply": "2024-05-16T12:24:52.958406Z",
     "shell.execute_reply.started": "2024-05-16T12:24:52.927248Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Map labels to integer values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Example output:\n",
    "# y_train_encoded = [1, 0, 2, 1, 0]\n",
    "\n",
    "# Perform one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1203b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:24:56.995149Z",
     "iopub.status.busy": "2024-05-16T12:24:56.994522Z",
     "iopub.status.idle": "2024-05-16T12:24:57.000999Z",
     "shell.execute_reply": "2024-05-16T12:24:57.000129Z",
     "shell.execute_reply.started": "2024-05-16T12:24:56.995115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 64, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51880513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:25:01.322846Z",
     "iopub.status.busy": "2024-05-16T12:25:01.322453Z",
     "iopub.status.idle": "2024-05-16T12:25:01.329325Z",
     "shell.execute_reply": "2024-05-16T12:25:01.328446Z",
     "shell.execute_reply.started": "2024-05-16T12:25:01.322817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d54123ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:25:04.862518Z",
     "iopub.status.busy": "2024-05-16T12:25:04.861781Z",
     "iopub.status.idle": "2024-05-16T12:25:04.869073Z",
     "shell.execute_reply": "2024-05-16T12:25:04.868065Z",
     "shell.execute_reply.started": "2024-05-16T12:25:04.862483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2b67a-92fd-47ee-9887-382d9b0e59f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/partiality-scores/unlabeled_tweets.csv')\n",
    "unlabeled_features = df['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617c705-de19-4d6d-8c8a-f3b678711add",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_length = len(unlabeled_features)\n",
    "max_len = max([ len(sentence.split()) for sentence in sentences ])\n",
    "X_unlabeled = np.zeros([ex_length,max_len+5,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3d9ec-878d-42b0-93fe-cf0ff9c218f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# russia 'روسيا', ukr 'اوكرانيا', war 'حرب' , putin 'بوتين', zelensky 'اوباما', belarus 'بيلاروسيا' , egypt 'قطر', us 'امريكا', gb \"بريطانيا\"\n",
    "for ex in range(ex_length):\n",
    "    words = unlabeled_features[ex].split()\n",
    "    position = 0\n",
    "    for word in words:\n",
    "        if word in sg_model.wv:\n",
    "            X_unlabeled[ex,position,:] = sg_model.wv[word]\n",
    "        position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d3c4909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:26:16.816499Z",
     "iopub.status.busy": "2024-05-16T12:26:16.815769Z",
     "iopub.status.idle": "2024-05-16T12:26:16.948607Z",
     "shell.execute_reply": "2024-05-16T12:26:16.947756Z",
     "shell.execute_reply.started": "2024-05-16T12:26:16.816461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">84,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m84,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m263,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │         \u001b[38;5;34m5,140\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m63\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352,851</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m352,851\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352,851</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m352,851\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add Bidirectional LSTM layers\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True), input_shape=(None, 100)))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n",
    "\n",
    "# Add Dense layer\n",
    "model.add(Dense(units=20, activation='relu'))\n",
    "\n",
    "# Add Softmax layer\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "loss= CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0.0,\n",
    "    axis=-1,\n",
    "    reduction=\"sum_over_batch_size\",\n",
    "    name=\"categorical_crossentropy\",\n",
    ")\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76bec447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:27:03.612532Z",
     "iopub.status.busy": "2024-05-16T12:27:03.611866Z",
     "iopub.status.idle": "2024-05-16T12:27:03.674110Z",
     "shell.execute_reply": "2024-05-16T12:27:03.673092Z",
     "shell.execute_reply.started": "2024-05-16T12:27:03.612497Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 34 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('best_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3f3c33e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:34:35.988094Z",
     "iopub.status.busy": "2024-05-16T12:34:35.987131Z",
     "iopub.status.idle": "2024-05-16T12:37:18.643319Z",
     "shell.execute_reply": "2024-05-16T12:37:18.642192Z",
     "shell.execute_reply.started": "2024-05-16T12:34:35.988057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m2812/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9795 - loss: 0.0561\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93180, saving model to best_weights.keras\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 19ms/step - accuracy: 0.9795 - loss: 0.0561 - val_accuracy: 0.9318 - val_loss: 0.2539\n",
      "Epoch 2/3\n",
      "\u001b[1m2812/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9830 - loss: 0.0467\n",
      "Epoch 2: val_accuracy did not improve from 0.93180\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 18ms/step - accuracy: 0.9830 - loss: 0.0467 - val_accuracy: 0.9268 - val_loss: 0.2894\n",
      "Epoch 3/3\n",
      "\u001b[1m2811/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9884 - loss: 0.0323\n",
      "Epoch 3: val_accuracy did not improve from 0.93180\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 18ms/step - accuracy: 0.9884 - loss: 0.0323 - val_accuracy: 0.9242 - val_loss: 0.3459\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint filepath with '.keras' extension\n",
    "checkpoint_filepath = 'best_weights.keras'\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                      save_best_only=True,\n",
    "                                      monitor='val_accuracy',\n",
    "                                      mode='max',  # 'max' for accuracy\n",
    "                                      verbose=1)\n",
    "\n",
    "# Train the model with ModelCheckpoint callback\n",
    "history = model.fit(x, y_train_one_hot,\n",
    "                    epochs=3,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5ce23-cd23-4c4e-8da9-cf3c5d459ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the substitute vector if we dropped the word\n",
    "drop_symbol = np.zeros(100)\n",
    "consistency_lambda = 0.01\n",
    "loss_fn = CategoricalCrossentropy(from_logits=False,label_smoothing=0.0,axis=-1,reduction=\"sum_over_batch_size\",name=\"categorical_crossentropy\")\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    # Train model on labeled data\n",
    "    history = model.fit(x, y_train_one_hot,epochs=1,batch_size=32,validation_split=0.1,callbacks=[checkpoint_callback])\n",
    "    \n",
    "    # Open a GradientTape context to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute predictions for labeled data\n",
    "        predictions = model(x)\n",
    "        # Compute original loss\n",
    "        loss_value = loss_fn(y_train_one_hot, predictions)\n",
    "        \n",
    "        # Generate four different views for each tweet in the unlabeled data by droping words\n",
    "        unlabeled_sequences_views = []\n",
    "        for sequence in X_unlabeled:\n",
    "            unlabeled_sequences_views.append(sequences)\n",
    "            for _ in range(3):\n",
    "                cross_view = sequence.copy()  # Make a copy of the original sequence\n",
    "                drop = np.random.choice(64, size=1, replace=False)[0]\n",
    "                cross_view[drop] = drop_symbol\n",
    "                unlabeled_sequences_views.append(cross_view)\n",
    "\n",
    "\n",
    "        # Compute predictions for unlabeled data using the model\n",
    "        unlabeled_preds = model.predict(unlabeled_sequences_views)\n",
    "\n",
    "\n",
    "        # Repeat the target labels to match the number of views for each sample\n",
    "        unlabeled_labels_repeated = tf.repeat(unlabeled_labels, repeats=4, axis=0)\n",
    "\n",
    "        # Compute consistency loss between predictions from different views\n",
    "        consistency_loss_value = tf.reduce_mean(tf.abs(unlabeled_preds[::4] - unlabeled_preds[1::4])) * consistency_lambda\n",
    "\n",
    "        # Compute total loss\n",
    "        total_loss = loss_value + consistency_loss_value\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "\n",
    "    # Update optimizer's parameters based on gradients\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Consistency Loss: {consistency_loss_value.numpy():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ea73a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:38:53.872168Z",
     "iopub.status.busy": "2024-05-16T12:38:53.871231Z",
     "iopub.status.idle": "2024-05-16T12:38:53.977087Z",
     "shell.execute_reply": "2024-05-16T12:38:53.976027Z",
     "shell.execute_reply.started": "2024-05-16T12:38:53.872131Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('best_weights.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9065c3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-16T11:41:30.797350Z",
     "iopub.status.idle": "2024-05-16T11:41:30.797668Z",
     "shell.execute_reply": "2024-05-16T11:41:30.797526Z",
     "shell.execute_reply.started": "2024-05-16T11:41:30.797513Z"
    }
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61a0ab71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T12:39:02.521406Z",
     "iopub.status.busy": "2024-05-16T12:39:02.520588Z",
     "iopub.status.idle": "2024-05-16T12:39:02.663860Z",
     "shell.execute_reply": "2024-05-16T12:39:02.662687Z",
     "shell.execute_reply.started": "2024-05-16T12:39:02.521366Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to synchronously create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msup_tweets200k_v2.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[0;32m--> 163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m    166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:137\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to synchronously create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "model.save('sup_tweets200k_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "491afd0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T13:07:33.974994Z",
     "iopub.status.busy": "2024-05-16T13:07:33.973849Z",
     "iopub.status.idle": "2024-05-16T13:07:34.440743Z",
     "shell.execute_reply": "2024-05-16T13:07:34.439706Z",
     "shell.execute_reply.started": "2024-05-16T13:07:33.974947Z"
    }
   },
   "outputs": [],
   "source": [
    "# lef 3al sentences we we for each word 7ot 1 fel index beta3 el word\n",
    "# we make adjustments for the decoder target data\n",
    "ex_length = 100000\n",
    "max_len = max([ len(sentence.split()) for sentence in sentences ])\n",
    "encoder_input_data = np.zeros([ex_length,max_len+5,100])\n",
    "# decoder_output_data = np.zeros([ex_length,max_len+5,100])\n",
    "# decoder_input_data = np.zeros([ex_length,max_len+5,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48422308",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = list(sentences[200000:].astype(str))\n",
    "ex_length = len(test_set)\n",
    "encoder_input_data = np.zeros([ex_length,max_len+5,100])\n",
    "for ex in range(ex_length):\n",
    "    words = test_set[ex].split()\n",
    "    position = 0\n",
    "    for word in words:\n",
    "        if word in sg_model.wv:\n",
    "            encoder_input_data[ex,position,:] = sg_model.wv[word]\n",
    "        position += 1       "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5012023,
     "sourceId": 8419277,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5008857,
     "sourceId": 8428354,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
